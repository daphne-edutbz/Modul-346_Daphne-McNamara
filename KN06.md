# KN06: Skalierung

## A) Installation App (50%)

### Reverse Proxy
Reverse Proxy ist ein Server, der Anfragen von Clients entgegennimmt und sie an einen internen Backend-Service weiterleitet.

In dieser Umgebung:
- .NET Web API läuft intern auf Port 5000
- Nginx hört auf Port 80
- Nginx leitet Anfragen an die Web API weiter

Vorteile eines Reverse Proxys:
- Benutzer greifen nur über Port 80 zu
- Interne Ports bleiben verborgen
- Grundlage für Load Balancer & Skalierung
- Zentrale Stelle für HTTPS, Logging, Security


### Mongo DB Atlas


#### MongoDB Atlas (SaaS)
Für diese Aufgabe wurde MongoDB Atlas als SaaS-Datenbank verwendet.
Dadurch ist kein eigener Datenbankserver notwendig und Wartung, Backups sowie Verfügbarkeit werden vom Anbieter übernommen.

**Einrichtung:**
1. Kostenloses MongoDB-Atlas-Konto erstellt
2. Cluster erstellt (ClusterDaphne)
3. Datenbank-User erstellt (KN06-Daphne)
4. IP-Zugriff erlaubt (temporär 0.0.0.0/0 für Tests)
5. Verbindung über mongodb+srv://...

Die Datenbank wird über die MongoDB Shell (mongosh) aus dem Cloud-Init Script befüllt.

<img width="955" height="511" alt="DBUser" src="https://github.com/user-attachments/assets/f9817739-9e83-4ccc-8b59-ab5d617f8f7c" />

<img width="958" height="509" alt="IPAccessList" src="https://github.com/user-attachments/assets/21f9aa30-8947-496b-8642-9732e7397e75" />



#### Collections

<img width="949" height="481" alt="MongoDBCollections" src="https://github.com/user-attachments/assets/501c437a-ce53-4928-9f87-2d22e6708022" />

Die Datenbank enthält mehrere Collections (z. B. `products`, `users`). Diese wurden über das Cloud-Init Script automatisch importiert.




### Swagger

**Zugriffspunkt:** `http://<Public-IP>/swagger`

<img width="959" height="523" alt="SwaggerUI" src="https://github.com/user-attachments/assets/30d2c629-12b2-4449-ad75-57716de614e7" />



**Endpoint:** `GET /Shop/GetProducts`

<img width="953" height="529" alt="GetProducts" src="https://github.com/user-attachments/assets/f16f5a69-4157-4ca8-948a-ba25470f771a" />




### Cloud-Init

**Welche(r) Teil(e) macht/machen hier überhaupt keinen Sinn in einer produktiven Umgebung?**


1. Passwörter im Klartext
- MongoDB Benutzer & Passwort sind im Cloud-Init sichtbar



2. Benutzer kann alle Befehle ohne Passwort ausführen

`
NOPASSWD:ALL
`




3. Root-Login möglich -> Sicherheitsrisiko

`disable_root: false`



4. Datenbank von überall erreichbar

`IP-Zugriff 0.0.0.0/0`


5. Kein HTTPS
- Kommunikation erfolgt unverschlüsselt


6. Datenbank-Import bei jedem Start
- Führt zu doppelten Daten bei Neustarts


In einer produktiven Umgebung würden:
- Secrets Manager
- eingeschränkte IP-Ranges
- HTTPS
- getrennte Build-/Deploy-Prozesse
verwendet werden




## B) Vertikale Skalierung  (10%)

### Erweiterung der Disk (EBS Volume)
- Disk von 8GB auf 20GB erweitern

#### Vorgehen:
1. Instanz stoppen
2. Elastic Block Store -> Volumes: EBS-Volume auswählen
3. Volume über „Modify Volume“ auf 20 GB vergrössern
4. Instanz wieder starten

Die Änderung ist nicht im laufenden Betrieb möglich -> Instanz MUSS gestoppt werden

#### Vorher:

(Ich habe das Bild von wie es vorher aussah nicht gefunden, aber so sah es aus bevor ich es geändert hab (ohne die Warnung da))

<img width="955" height="478" alt="VolumeVorher" src="https://github.com/user-attachments/assets/f4447d8a-0a4b-4fc3-8c79-8999199cab09" />

#### Nachher:

<img width="756" height="311" alt="VolumeNachher" src="https://github.com/user-attachments/assets/4da481f3-3ddc-4fb1-aa8c-3cc4cd6f3d12" />


### Änderung des Instanztyps
- Web-Instanz wurde von einem kleineren Typ auf t2.medium skaliert, um mehr CPU und RAM zur Verfügung zu haben

#### Vorgehen:
1. Instanz stoppen
2. Instance Type auf t2.medium ändern (Instanz auswählen -> Actions -> Instance Settings -> Change Instance type)
3. Instanz wieder starten

Auch diese Änderung ist nicht im laufenden Betrieb möglich, da CPU und RAM nur im gestoppten Zustand geändert werden können.

#### Vorher

<img width="760" height="388" alt="TypeVorher" src="https://github.com/user-attachments/assets/b3bb391d-8f98-47d2-9ab0-c5eb821af6a6" />

#### Nachher

<img width="950" height="464" alt="TypeNachher" src="https://github.com/user-attachments/assets/df913fe8-872a-4ec1-98c8-f193e27f8a20" />


## C) Horizontale Skalierung (20%)

### Ausgangslage
Applikation lief ursprünglich auf einer einzelnen EC2-Instanz und wurde direkt über eine Public IP-Adresse angesprochen.
Diese Architektur ist weder hochverfügbar noch skalierbar.

**Ziel:** Applikation horizontal zu skalieren, indem mehrere Webserver parallel betrieben und über einen Load Balancer erreichbar gemacht werden.

### DNS (Domain Name System)

#### DNS Erklärung
- übersetzt Domainnamen in IP-Adressen, damit Clients wissen, zu welchem Server sie sich verbinden müssen.

**Beispiel:**
`www.google.com -> IP-Adresse`
1. Ich suche Website (Google), gebe diese URL ein https://www.google.com
2. Mein Gerät macht Anfrage an DNS-Server, fragt "Welche IP-Adresse gehört zu dieser URL"
3. DNS-Server gibt IP-Adresse zurück
4. Mein Gerät verbindet sich über erhaltene IP-Adresse zum Google Server

#### DNS in dieser Umgebung
- Application Load Balancer stellt bereits einen DNS-Namen zur Verfügung:
  
  `KN06-Daphne-709760289.us-east-1.elb.amazonaws.com`

- Dieser DNS-Name verweist dynamisch auf den Load Balancer und dessen IP-Adressen


### Load Balancer URL
Für die horizontale Skalierung wurde ein Application Load Balancer (ALB) erstellt

Load Balancer DNS (AWS):
`KN06-Daphne-709760289.us-east-1.elb.amazonaws.com`

- Diese URL ist nun der einzige Einstiegspunkt zur Applikation
- Webserver werden nicht mehr direkt über ihre Public IPs angesprochen

### Swagger Aufruf über Load-Balancer

``http://KN06-Daphne-709760289.us-east-1.elb.amazonaws.com/swagger/index.html``

- Aufruf funktioniert identisch wie zuvor über die Instanz-IP, jedoch nun lastverteilt über mehrere Webserver.

#### Endpoint-Test über Load Balancer
- Auch der API-Endpoint funktioniert über die Load-Balancer-URL:
``
GET /Shop/GetProducts
``
- Load Balancer verteilt Anfrage automatisch auf eine der beiden Webserver-Instanzen


<img width="959" height="531" alt="LoadBalancerURL" src="https://github.com/user-attachments/assets/eee76e62-6e41-4e08-a3a0-60edd5731568" />


### Erstellte Komponenten & Vorgehen

#### Webserver-Instanzen
- Zwei EC2-Instanzen basierend auf derselben AMI

##### Konfiguration:
- .NET Web API auf Port 5000
- Nginx als Reverse Proxy auf Port 80

Beide Instanzen sind funktional identisch und austauschbar

##### Instanz 1:
<img width="763" height="398" alt="AMIInstance1" src="https://github.com/user-attachments/assets/2e511645-a04d-41b8-ae3c-4a93c5fde9a6" />

<img width="924" height="446" alt="AMIInstance1GetProducts" src="https://github.com/user-attachments/assets/cca05f2e-0d2e-422c-be8b-0f28c39a0392" />



##### Instanz 2:

<img width="746" height="395" alt="AMIInstance2" src="https://github.com/user-attachments/assets/585d996e-ab5c-44c0-93bb-85300716a82a" />

<img width="954" height="535" alt="AMIInstance2GetProducts" src="https://github.com/user-attachments/assets/a6403eb0-6b2b-4d7d-b29d-00e7dd5378bc" />


#### Target Group
- Typ: Instance
- Protokoll: HTTP
- Port: 80
- Enthält beide Webserver-Instanzen

Health Check:
- Path: /swagger/index.html
- Erfolgreich bei HTTP 200
- Nur gesunde Instanzen erhalten Traffic


<img width="743" height="371" alt="TargetGroup" src="https://github.com/user-attachments/assets/bb4cf08c-96db-4520-ab9d-4c7c6f1b2f9e" />


#### Load Balancer
- Typ: Application Load Balancer
- Internet-facing
- Listener:
    - HTTP auf Port 80
    - Weiterleitung an die Target Group

Load Balancer übernimmt:
- Lastverteilung
- Health Checks
- DNS-Zugriffspunkt


<img width="747" height="376" alt="LoadBalancer" src="https://github.com/user-attachments/assets/0c2ec2f2-b915-45ad-bb68-e63971550bd4" />


#### Sicherheitsgruppen

##### Load Balancer Security Group
Inbound:
- HTTP (80) von 0.0.0.0/0


##### Webserver Security Group
Inbound:
- HTTP (80) nur vom Load Balancer
- SSH (22) nur von der eigenen IP

Direkter Zugriff auf Webserver aus dem Internet ist nicht mehr notwendig


## D) Auto Scaling (20%)

Für die horizontale Skalierung wurde eine Auto Scaling Group erstellt, welche mit dem bestehenden Application Load Balancer verbunden ist.
Die Auto Scaling Group stellt sicher, dass jederzeit mindestens zwei Webserver verfügbar sind und bei Bedarf automatisch neue Instanzen
gestartet werden können (maximal fünf).

<img width="737" height="382" alt="ASGDetails" src="https://github.com/user-attachments/assets/0ed2bc7f-51ff-4f4e-b03b-db779ac23518" />

### Konfiguration
- Launch Template: KN06-Daphne-WebTemplate
  - AMI: KN06 (C) - Daphne
  - Instance Type: t2.medium
  - Security Group: KN06 - Daphne (c - nodeBalancer)

- Auto Scaling Group: KN06-Daphne-ASG
  - Desired Capacity: 2
  - Minimum Capacity: 2
  - Maximum Capacity: 5
  - Availability Zones: 3
  - Health Checks: EC2 + Elastic Load Balancer
  - Health Check Grace Period: 300 Sekunden

- Integration
  - Application Load Balancer: KN06-Daphne
  - Target Group: KN06-Daphne



### Test der Auto-Skalierung

<img width="787" height="277" alt="2ASGInstances" src="https://github.com/user-attachments/assets/d67b78ed-5672-469d-99f5-cf70d5fd6ac9" />


Zur Überprüfung der Auto-Skalierung wurde eine laufende Webserver-Instanz manuell gestoppt. Die Auto Scaling Group erkannte den Ausfall über die Health Checks und startete automatisch eine neue EC2-Instanz.


<img width="716" height="383" alt="ASGActivityHistory" src="https://github.com/user-attachments/assets/0c1de5d7-266b-483a-aaf4-1ee91fbe55e7" />


In der Activity-History der Auto Scaling Group:
- neue Instanz als Ersatz gestartet
- Währenddessen blieb die Applikation über die Load-Balancer-URL erreichbar.





